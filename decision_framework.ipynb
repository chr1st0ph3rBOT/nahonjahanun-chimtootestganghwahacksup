{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c2bad6",
   "metadata": {},
   "source": [
    "현진: 호기심\n",
    "원준: 플래그/폴리시\n",
    "나: 예측 기반 오차/오류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be955d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러 기반\n",
    "\n",
    "def error():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측기반 오차\n",
    "\n",
    "# 전체 파라미터를 트리 구조로 바꾸는 코드\n",
    "\n",
    "\n",
    "def prophecy():\n",
    "    next_output_value_type = 0 # 여기에 예측 코드 넣으면 됨\n",
    "    return next_output_value_type\n",
    "\n",
    "def main(prophecy_out, real_out):\n",
    "    # 유전 거리 계산 코드\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ded50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#뭐가 어떻게 될지 몰라 일단 함수 기틀만 만들어 보았음.\n",
    "'''우선, 호기심 보상 함수를 두가지 버전으로 만들어보앗는데,\n",
    "첫번째로는 은세가 말했던 스텝이 진행될 수록 기틀이 잡히니까 호기심 보상을 자동적으로 감쇠시키는 버전,\n",
    "두번째로는 후반에 가서도 호기심 보상을 감소시키지 않을 수 있게 최악의 상태일떄 음의 보상으로 패치하는 버전으로 만들었음.\n",
    "'''\n",
    "\n",
    "\n",
    "#자동 감쇠형 호기심 함수\n",
    "def curiosity_reward_decay(step, base_reward=1.0, decay_rate=0.005):\n",
    "    \"\"\"\n",
    "    호기심 보상이 스텝(step)에 따라 자연스럽게 감소하는 함수.\n",
    "    예: 초반엔 높은 보상, 후반으로 갈수록 점점 감소.\n",
    "\n",
    "    R_c(t) = base_reward * (1 - decay_rate * step)\n",
    "    (단, 0보다 작으면 0으로 제한)\n",
    "    \"\"\"\n",
    "    reward = max(base_reward * (1 - decay_rate * step), 0)\n",
    "    return reward\n",
    "# 최악의 경우에 음의 보상을 주는 호기심 함수\n",
    "def curiosity_reward_with_penalty(is_redundant=False, is_error=False, step=0,\n",
    "                                  base_reward=1.0, penalty_redundant=0.3, penalty_error=0.5,\n",
    "                                  decay_rate=0.003):\n",
    "    \"\"\"\n",
    "    스텝 수에 따른 호기심 보상 감소 + 잘못된 행동에 대한 음의 보상 적용.\n",
    "\n",
    "    Parameters:\n",
    "    - is_redundant: 이미 여러 번 시도된 동일한 행동 수행시 True\n",
    "    - is_error: 시스템 오류를 유발하는 탐색시 True\n",
    "    - step: 현재 스텝 수\n",
    "    - base_reward: 기본 호기심 보상\n",
    "    - penalty_redundant: 반복 행동에 부여할 음의 보상 강도\n",
    "    - penalty_error: 심각한 오류 행위에 부여할 음의 보상 강도\n",
    "    - decay_rate: 스텝에 따른 감쇠 비율\n",
    "    \"\"\"\n",
    "    reward = max(base_reward * (1 - decay_rate * step), 0)\n",
    "    if is_redundant:\n",
    "        reward -= penalty_redundant\n",
    "    if is_error:\n",
    "        reward -= penalty_error\n",
    "    return max(reward, -1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533eba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG 보상 함수 (기틀 버전)\n",
    "def flag_reward(flag_str, method, known_flags, found_paths):\n",
    "    \"\"\"\n",
    "    플래그를 찾았을 때 보상을 부여하는 함수 (구조만 설계함)\n",
    "    - flag_str: 발견된 플래그 문자열\n",
    "    - method: 플래그를 찾은 탐색 경로(명령/행동 이름)\n",
    "    - known_flags: 미리 등록된 정답 플래그 해시 목록(dict)\n",
    "    - found_paths: 이미 찾은 플래그 및 경로 기록용 dict\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    # 1) 해시 비교 (예: SHA256)\n",
    "    flag_hash = hash(flag_str)\n",
    "    for key, val in known_flags.items():\n",
    "        if flag_hash == val:\n",
    "            # 2) 이미 찾았던 플래그인가?\n",
    "            if method not in found_paths.get(key, []):\n",
    "                reward = 5   # 기본 보상\n",
    "                reward += 2  # 새로운 경로로 찾은 보너스\n",
    "                found_paths.setdefault(key, []).append(method)\n",
    "            else:\n",
    "                reward = 5   # 동일 경로는 기본 보상만\n",
    "            break\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy (ε-탐욕 기반)\n",
    "def policy_update(actions, Q, epsilon=0.1, alpha=0.1, last_action=None, reward=0):\n",
    "    \"\"\"\n",
    "    간단한 ε-탐욕 정책 함수 (기틀 버전)\n",
    "    - actions: 가능한 행동 리스트\n",
    "    - Q: 행동 가치 테이블 (dict)\n",
    "    - epsilon: 무작위 탐색 확률\n",
    "    - alpha: 학습률\n",
    "    - last_action: 직전 수행한 행동\n",
    "    - reward: 직전 보상\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Q값 업데이트 (최근 보상 반영)\n",
    "    if last_action is not None:\n",
    "        old_value = Q.get(last_action, 0)\n",
    "        Q[last_action] = old_value + alpha * (reward - old_value)\n",
    "\n",
    "    # ε-탐욕 정책으로 다음 행동 선택\n",
    "    if random.random() < epsilon:\n",
    "        next_action = random.choice(actions)\n",
    "    else:\n",
    "        next_action = max(actions, key=lambda a: Q.get(a, 0))\n",
    "\n",
    "    return next_action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd32",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
