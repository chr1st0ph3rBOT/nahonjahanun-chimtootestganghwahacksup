{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c2bad6",
   "metadata": {},
   "source": [
    "현진: 호기심\n",
    "원준: 플래그/폴리시\n",
    "나: 예측 기반 오차/오류\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "에피소드 기반 보상은 나중에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be955d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': -0.2, 'reward': 0.8, 'error_detected': True, 'error_type': 'hostname_resolution_error', 'suggested_fix': '대상 호스트 이름이 올바른지 확인하거나 IP 주소로 직접 입력하세요.', 'auto_fixable': True}\n"
     ]
    }
   ],
   "source": [
    "#에러기반\n",
    "import re\n",
    "\n",
    "#  오류 유형별 penalty 상수 선언\n",
    "PENALTY_DICT = {\n",
    "    'hostname_resolution_error':   -0.2,  # 호스트 이름 해석 실패\n",
    "    'device_access_error':         -0.3,  # 장치 접근 권한/불가\n",
    "    'network_unreachable':         -0.25, # 대상 네트워크 접근 불가\n",
    "    'invalid_target':              -0.15, # 입력 타겟값 부적절\n",
    "    'nmap_internal_error':         -0.4   # 내부 오류(비정상 종료 등)\n",
    "}\n",
    "MIN_REWARD = -1.0  # 최저 보상값 (절대치)\n",
    "MAX_REWARD = 1.0   # 최고 보상값\n",
    "\n",
    "def error(output_log):\n",
    "    \"\"\"\n",
    "    nmap 실행 중 발생한 오류 메시지를 분석하고,\n",
    "    자동으로 수정 가능한 경우 대응 조치를 제안하거나 보정한다.\n",
    "\n",
    "    Parameters:\n",
    "    - output_log (str): nmap 실행 결과 로그 문자열\n",
    "\n",
    "    Returns:\n",
    "    - dict: {\n",
    "        'error_detected': bool,      # 오류 존재 여부\n",
    "        'error_type': str,           # 오류 유형 식별(키)\n",
    "        'suggested_fix': str | None, # 제안된 수정 또는 재시도 방안\n",
    "        'auto_fixable': bool         # 자동 수정 가능 여부\n",
    "      }\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'error_detected': False,\n",
    "        'error_type': None,\n",
    "        'suggested_fix': None,\n",
    "        'auto_fixable': False\n",
    "    }\n",
    "    log = output_log.lower()\n",
    "\n",
    "    # 호스트 이름 해석 실패\n",
    "    if re.search(r\"(unable to resolve hostname|dns resolution failed|error resolving name)\", log):\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'hostname_resolution_error',\n",
    "            'suggested_fix': '대상 호스트 이름이 올바른지 확인하거나 IP 주소로 직접 입력하세요.',\n",
    "            'auto_fixable': True\n",
    "        })\n",
    "    #  장치 접근 불가\n",
    "    elif \"failed to open device\" in log:\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'device_access_error',\n",
    "            'suggested_fix': '루트 권한으로 다시 실행하거나 네트워크 인터페이스 이름을 명시하세요. (예: nmap -e eth0 ...)',\n",
    "            'auto_fixable': False\n",
    "        })\n",
    "    #  대상 접근 불가\n",
    "    elif \"no route to host\" in log:\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'network_unreachable',\n",
    "            'suggested_fix': '대상 네트워크 연결 상태를 점검하고, 방화벽 또는 VPN 설정을 확인하세요.',\n",
    "            'auto_fixable': False\n",
    "        })\n",
    "    #  유효하지 않은 대상\n",
    "    elif \"not a valid target\" in log:\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'invalid_target',\n",
    "            'suggested_fix': '입력한 대상 형식(IP, CIDR 등)을 다시 확인하세요.',\n",
    "            'auto_fixable': True\n",
    "        })\n",
    "    #  nmap 내부 오류 (일반적 종료)\n",
    "    elif re.search(r\"(nmap error|quitting)\", log):\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'nmap_internal_error',\n",
    "            'suggested_fix': 'nmap 명령 구문과 옵션을 다시 확인하거나 -d(디버그 모드)로 재실행하세요.',\n",
    "            'auto_fixable': False\n",
    "        })\n",
    "    #  오류 없음\n",
    "    else:\n",
    "        result.update({\n",
    "            'error_detected': False,\n",
    "            'error_type': None,\n",
    "            'suggested_fix': None,\n",
    "            'auto_fixable': False\n",
    "        })\n",
    "    return result\n",
    "\n",
    "def calc_penalty(log):\n",
    "    \"\"\"\n",
    "    error() 함수로 식별된 오류 유형에 따라 해당 penalty만큼\n",
    "    보상을 감소시키는 단일 책임 함수. 적용 penalty는 최저/최고 보상값 범위 내로 제한.\n",
    "    \"\"\"\n",
    "    err = error(log)  # 오류 분석 실행\n",
    "    # 오류 유형이 발견되면 PENALTY_DICT 값을 적용, 아니면 0(감점 없음)\n",
    "    penalty = PENALTY_DICT.get(err['error_type'], 0.0)\n",
    "    # 총 보상(예: 1.0에서 penalty만큼 빼기, 실제 시스템에서는 누적 보상/스텝별 반영)\n",
    "    reward = max(MIN_REWARD, min(MAX_REWARD, 1.0 + penalty))\n",
    "    # 상세 결과표와 계산된 보상 함께 반환\n",
    "    return {'penalty': penalty, 'reward': reward, **err}\n",
    "\n",
    "# 예시 사용\n",
    "log1 = \"Nmap scan report: Unable to resolve hostname example.local\"\n",
    "print(calc_penalty(log1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde928c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:11] START      | Phylo reward demo (no pytest)\n",
      "[18:16:11] CASE       | 거리 d 테스트\n",
      "[18:16:11] TEST       | dist(same leaf)              -> PASS (got=0, expect=0)\n",
      "[18:16:11] TEST       | dist(sibling leaves)         -> PASS (got=2, expect=2)\n",
      "[18:16:11] TEST       | dist(cousin leaves)          -> PASS (got=4, expect=4)\n",
      "[18:16:11] CASE       | LCA 테스트\n",
      "[18:16:11] TEST       | lca(status siblings)         -> PASS (got='git:status', expect='git:status')\n",
      "[18:16:11] TEST       | lca(status vs commit)        -> PASS (got='git', expect='git')\n",
      "[18:16:11] CASE       | 보상 r=1/(1+d) 테스트\n",
      "[18:16:11] TEST       | reward(d=0)                  -> PASS (got=1.000000000000, expect=1.000000000000)\n",
      "[18:16:11] TEST       | reward(d=2)                  -> PASS (got=0.333333333333, expect=0.333333333333)\n",
      "[18:16:11] TEST       | reward(d=4)                  -> PASS (got=0.200000000000, expect=0.200000000000)\n",
      "[18:16:11] CASE       | 케이스별 상수 보상(테이블)\n",
      "[18:16:11] TEST       | case-weight(d=2)             -> PASS (got=0.400000000000, expect=0.400000000000)\n",
      "[18:16:11] TEST       | case-weight(d=4)             -> PASS (got=0.200000000000, expect=0.200000000000)\n",
      "[18:16:11] SUMMARY    | d=0 -> r=1.000 | d=2 -> r=0.333 (or 0.400 by table) | d=4 -> r=0.200\n",
      "[18:16:11] DONE       | All checks complete.\n"
     ]
    }
   ],
   "source": [
    "# phylo_reward_demo.py\n",
    "from typing import Dict, Optional, Tuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "Node = str\n",
    "\n",
    "# -----------------------------\n",
    "# 간단 트리: LCA/거리/보상\n",
    "# -----------------------------\n",
    "class SimpleTree:\n",
    "    def __init__(self, parent: Dict[Node, Optional[Node]], depth: Dict[Node, int]):\n",
    "        self.parent = parent\n",
    "        self.depth = depth\n",
    "\n",
    "    def _align_depth(self, u: Node, v: Node) -> Tuple[Node, Node]:\n",
    "        du, dv = self.depth[u], self.depth[v]\n",
    "        while du > dv:\n",
    "            u = self.parent[u]; du -= 1\n",
    "        while dv > du:\n",
    "            v = self.parent[v]; dv -= 1\n",
    "        return u, v\n",
    "\n",
    "    def lca(self, u: Node, v: Node) -> Node:\n",
    "        u, v = self._align_depth(u, v)\n",
    "        path = []\n",
    "        while u != v:\n",
    "            path.append((u, v))\n",
    "            u = self.parent[u]\n",
    "            v = self.parent[v]\n",
    "        return u\n",
    "\n",
    "    def dist(self, u: Node, v: Node) -> int:\n",
    "        a = self.lca(u, v)\n",
    "        return (self.depth[u] - self.depth[a]) + (self.depth[v] - self.depth[a])\n",
    "\n",
    "def reward_inverse_distance(d: int) -> float:\n",
    "    return 1.0 / (1 + d)\n",
    "\n",
    "def reward_with_case_weights(d: int, weights: Dict[int, float]) -> float:\n",
    "    return weights.get(d, 1.0 / (1 + d))\n",
    "\n",
    "# -----------------------------\n",
    "# 로거(깔끔한 콘솔 출력)\n",
    "# -----------------------------\n",
    "def log(section: str, msg: str = \"\"):\n",
    "    ts = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{ts}] {section:<10} | {msg}\")\n",
    "\n",
    "def assert_eq(name: str, got, expect):\n",
    "    ok = got == expect\n",
    "    status = \"PASS\" if ok else \"FAIL\"\n",
    "    log(\"TEST\", f\"{name:<28} -> {status} (got={got!r}, expect={expect!r})\")\n",
    "    return ok\n",
    "\n",
    "def assert_close(name: str, got: float, expect: float, tol: float = 1e-9):\n",
    "    ok = abs(got - expect) <= tol * max(1.0, abs(expect))\n",
    "    status = \"PASS\" if ok else \"FAIL\"\n",
    "    log(\"TEST\", f\"{name:<28} -> {status} (got={got:.12f}, expect={expect:.12f})\")\n",
    "    return ok\n",
    "\n",
    "# -----------------------------\n",
    "# 데모 트리(루트→프로그램→명령어→리프)\n",
    "# 깊이: R(0) -> P(1) -> C(2) -> L(3)\n",
    "# -----------------------------\n",
    "def build_demo_tree() -> SimpleTree:\n",
    "    parent = {\n",
    "        \"R\": None,\n",
    "        \"git\": \"R\",\n",
    "        \"git:status\": \"git\",\n",
    "        \"git:commit\": \"git\",\n",
    "        # 리프\n",
    "        \"L_status_v1\": \"git:status\",\n",
    "        \"L_status_v2\": \"git:status\",\n",
    "        \"L_commit_simple\": \"git:commit\",\n",
    "    }\n",
    "    depth = {\n",
    "        \"R\": 0,\n",
    "        \"git\": 1,\n",
    "        \"git:status\": 2,\n",
    "        \"git:commit\": 2,\n",
    "        \"L_status_v1\": 3,\n",
    "        \"L_status_v2\": 3,\n",
    "        \"L_commit_simple\": 3,\n",
    "    }\n",
    "    return SimpleTree(parent, depth)\n",
    "\n",
    "# -----------------------------\n",
    "# 시나리오 실행 & 로깅\n",
    "# -----------------------------\n",
    "def run_demo():\n",
    "    log(\"START\", \"Phylo reward demo (no pytest)\")\n",
    "    tree = build_demo_tree()\n",
    "\n",
    "    # 거리 테스트\n",
    "    log(\"CASE\", \"거리 d 테스트\")\n",
    "    d00 = tree.dist(\"L_status_v1\", \"L_status_v1\")  # 0\n",
    "    d02 = tree.dist(\"L_status_v1\", \"L_status_v2\")  # 2 (형제)\n",
    "    d04 = tree.dist(\"L_status_v1\", \"L_commit_simple\")  # 4 (사촌, LCA=git)\n",
    "\n",
    "    assert_eq(\"dist(same leaf)\", d00, 0)\n",
    "    assert_eq(\"dist(sibling leaves)\", d02, 2)\n",
    "    assert_eq(\"dist(cousin leaves)\", d04, 4)\n",
    "\n",
    "    # LCA 테스트\n",
    "    log(\"CASE\", \"LCA 테스트\")\n",
    "    a1 = tree.lca(\"L_status_v1\", \"L_status_v2\")\n",
    "    a2 = tree.lca(\"L_status_v1\", \"L_commit_simple\")\n",
    "    assert_eq(\"lca(status siblings)\", a1, \"git:status\")\n",
    "    assert_eq(\"lca(status vs commit)\", a2, \"git\")\n",
    "\n",
    "    # 보상: 역수형\n",
    "    log(\"CASE\", \"보상 r=1/(1+d) 테스트\")\n",
    "    r0 = reward_inverse_distance(d00)  # 1.0\n",
    "    r2 = reward_inverse_distance(d02)  # 1/3\n",
    "    r4 = reward_inverse_distance(d04)  # 1/5\n",
    "    assert_close(\"reward(d=0)\", r0, 1.0)\n",
    "    assert_close(\"reward(d=2)\", r2, 1.0/3.0)\n",
    "    assert_close(\"reward(d=4)\", r4, 1.0/5.0)\n",
    "\n",
    "    # 보상: 케이스별 상수(weight 테이블)\n",
    "    log(\"CASE\", \"케이스별 상수 보상(테이블)\")\n",
    "    weights = {0: 1.0, 2: 0.40, 3: 0.22}  # 예시: d=2는 0.40으로 덮어쓰기\n",
    "    rw2 = reward_with_case_weights(d02, weights)\n",
    "    rw4 = reward_with_case_weights(d04, weights)  # 테이블에 없으니 역수형 폴백(0.2)\n",
    "    assert_close(\"case-weight(d=2)\", rw2, 0.40)\n",
    "    assert_close(\"case-weight(d=4)\", rw4, 1.0/5.0)\n",
    "\n",
    "    # 요약 출력\n",
    "    log(\"SUMMARY\", f\"d=0 -> r={r0:.3f} | d=2 -> r={r2:.3f} (or {rw2:.3f} by table) | d=4 -> r={r4:.3f}\")\n",
    "\n",
    "    log(\"DONE\", \"All checks complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ded50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#뭐가 어떻게 될지 몰라 일단 함수 기틀만 만들어 보았음.\n",
    "'''우선, 호기심 보상 함수를 두가지 버전으로 만들어보앗는데,\n",
    "첫번째로는 은세가 말했던 스텝이 진행될 수록 기틀이 잡히니까 호기심 보상을 자동적으로 감쇠시키는 버전,\n",
    "두번째로는 후반에 가서도 호기심 보상을 감소시키지 않을 수 있게 최악의 상태일떄 음의 보상으로 패치하는 버전으로 만들었음.\n",
    "'''\n",
    "\n",
    "\n",
    "#자동 감쇠형 호기심 함수\n",
    "def curiosity_reward_decay(step, base_reward=1.0, decay_rate=0.005):\n",
    "    \"\"\"\n",
    "    호기심 보상이 스텝(step)에 따라 자연스럽게 감소하는 함수.\n",
    "    예: 초반엔 높은 보상, 후반으로 갈수록 점점 감소.\n",
    "\n",
    "    R_c(t) = base_reward * (1 - decay_rate * step)\n",
    "    (단, 0보다 작으면 0으로 제한)\n",
    "    \"\"\"\n",
    "    reward = max(base_reward * (1 - decay_rate * step), 0)\n",
    "    return reward\n",
    "# 최악의 경우에 음의 보상을 주는 호기심 함수\n",
    "def curiosity_reward_with_penalty(is_redundant=False, is_error=False, step=0,\n",
    "                                  base_reward=1.0, penalty_redundant=0.3, penalty_error=0.5,\n",
    "                                  decay_rate=0.003):\n",
    "    \"\"\"\n",
    "    스텝 수에 따른 호기심 보상 감소 + 잘못된 행동에 대한 음의 보상 적용.\n",
    "\n",
    "    Parameters:\n",
    "    - is_redundant: 이미 여러 번 시도된 동일한 행동 수행시 True ---\n",
    "    - is_error: 시스템 오류를 유발하는 탐색시 True >>>\n",
    "    - step: 현재 스텝 수 >>>\n",
    "    - base_reward: 기본 호기심 보상 >>>\n",
    "    - penalty_redundant: 반복 행동에 부여할 음의 보상 강도\n",
    "    - penalty_error: 심각한 오류 행위에 부여할 음의 보상 강도\n",
    "    - decay_rate: 스텝에 따른 감쇠 비율\n",
    "    \"\"\"\n",
    "    reward = max(base_reward * (1 - decay_rate * step), 0)\n",
    "    if is_redundant:\n",
    "        reward -= penalty_redundant\n",
    "    if is_error:\n",
    "        reward -= penalty_error\n",
    "    return max(reward, -1.0)\n",
    "\n",
    "\n",
    "\n",
    "# 새로운 행동 -> 보상\n",
    "\n",
    "# 시간 감쇠\n",
    "\n",
    "# 연속 반복 페널티\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533eba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import hmac\n",
    "\n",
    "def flag_reward(flag_str, known_flags, big_reward=1000.0):\n",
    "    \"\"\"\n",
    "    플래그 정답 여부에 따른 스칼라 보상 계산 함수 (순수 함수)\n",
    "    \n",
    "    파라미터:\n",
    "    - flag_str     : (str) 제출/발견된 플래그 원문\n",
    "    - known_flags  : (dict) {\"FLAG\": \"<sha256_hex>\"}  (정답 해시 딕셔너리)\n",
    "    - big_reward   : (float) 정답시 지급 보상 크기 (기본 1000.0)\n",
    "    \n",
    "    반환값:\n",
    "    - reward       : (float) 정답 시 big_reward, 아니면 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    # 입력 검증\n",
    "    if not isinstance(flag_str, str):\n",
    "        return 0.0\n",
    "\n",
    "    # 제출 플래그 해시 계산\n",
    "    submitted_hash = hashlib.sha256(flag_str.strip().encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    # 등록된 정답 해시 확인\n",
    "    try:\n",
    "        expected_key, expected_hash = next(iter(known_flags.items()))\n",
    "    except StopIteration:\n",
    "        return 0.0\n",
    "\n",
    "    # 안전 비교\n",
    "    is_correct = hmac.compare_digest(submitted_hash, expected_hash)\n",
    "\n",
    "    # 보상 산출 (오직 정답만 지급, 외부 상태 미변경)\n",
    "    reward = float(big_reward) if is_correct else 0.0\n",
    "\n",
    "    # [PARSE_HOOK]  \n",
    "    # 필요하면 여기서 플래그 내 추가 메타데이터(REWARD= 등) 파싱 확장\n",
    "    # 예시:\n",
    "    #   parsed_reward = parse_reward_from_string(flag_str)\n",
    "    #   if is_correct and parsed_reward is not None:\n",
    "    #       reward += float(parsed_reward)\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd87047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():   \n",
    "    error_reward()\n",
    "    prophecy_reward()\n",
    "    flag_reward()\n",
    "    curiosity()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy (ε-탐욕 기반)\n",
    "def policy_update(actions, Q, epsilon=0.1, alpha=0.1, last_action=None, reward=0):\n",
    "    \"\"\"\n",
    "    간단한 ε-탐욕 정책 함수 (기틀 버전)\n",
    "    - actions: 가능한 행동 리스트\n",
    "    - Q: 행동 가치 테이블 (dict)\n",
    "    - epsilon: 무작위 탐색 확률\n",
    "    - alpha: 학습률\n",
    "    - last_action: 직전 수행한 행동\n",
    "    - reward: 직전 보상\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Q값 업데이트 (최근 보상 반영)\n",
    "    if last_action is not None:\n",
    "        old_value = Q.get(last_action, 0)\n",
    "        Q[last_action] = old_value + alpha * (reward - old_value)\n",
    "\n",
    "    # ε-탐욕 정책으로 다음 행동 선택\n",
    "    if random.random() < epsilon:\n",
    "        next_action = random.choice(actions)\n",
    "    else:\n",
    "        next_action = max(actions, key=lambda a: Q.get(a, 0))\n",
    "\n",
    "    return next_action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
