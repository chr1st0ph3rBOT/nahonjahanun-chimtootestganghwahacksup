{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c2bad6",
   "metadata": {},
   "source": [
    "현진: 호기심\n",
    "원준: 플래그/폴리시\n",
    "나: 예측 기반 오차/오류\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "에피소드 기반 보상은 나중에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be955d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러 기반\n",
    "\n",
    "def error():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "Node = str\n",
    "\n",
    "class SimpleTree:\n",
    "    def __init__(self, parent: Dict[Node, Optional[Node]], depth: Dict[Node, int]):\n",
    "        self.parent = parent          # 각 노드 -> 부모\n",
    "        self.depth  = depth           # 각 노드 깊이(루트=0)\n",
    "\n",
    "    def lca(self, u: Node, v: Node) -> Node:\n",
    "        du, dv = self.depth[u], self.depth[v]\n",
    "        # 깊이를 맞춘다\n",
    "        while du > dv:\n",
    "            u = self.parent[u]; du -= 1\n",
    "        while dv > du:\n",
    "            v = self.parent[v]; dv -= 1\n",
    "        # 함께 올리며 만나는 지점 = LCA\n",
    "        while u != v:\n",
    "            u = self.parent[u]\n",
    "            v = self.parent[v]\n",
    "        return u\n",
    "\n",
    "    def dist(self, u: Node, v: Node) -> int:\n",
    "        a = self.lca(u, v)\n",
    "        return (self.depth[u] - self.depth[a]) + (self.depth[v] - self.depth[a])\n",
    "\n",
    "def reward_inverse_distance(d: int) -> float:\n",
    "    \"\"\"가중 없이 r = 1/(1+d)\"\"\"\n",
    "    return 1.0 / (1 + d)\n",
    "\n",
    "def reward_with_case_weights(d: int, weights: Dict[int, float]) -> float:\n",
    "    \"\"\"\n",
    "    d가 (0,1,2,3) 등 소수 케이스만 나온다면 if/매핑으로 처리.\n",
    "    테이블에 없으면 기본 역수형으로 폴백.\n",
    "    \"\"\"\n",
    "    if d in weights:\n",
    "        return weights[d]\n",
    "    return 1.0 / (1 + d)\n",
    "\n",
    "# ---- 사용 예시 ----\n",
    "# parent/depth은 한 번만 준비해두면 됨.\n",
    "# 예: 루트 R(0) - 프로그램 P(1) - 명령어 C(2) - 리프 L(3)\n",
    "parent = {\n",
    "    \"R\": None,\n",
    "    \"git\": \"R\",\n",
    "    \"git:status\": \"git\",\n",
    "    \"git:commit\": \"git\",\n",
    "    \"L_status_v1\": \"git:status\",\n",
    "    \"L_status_v2\": \"git:status\",\n",
    "    \"L_commit_simple\": \"git:commit\",\n",
    "}\n",
    "depth = {\"R\":0, \"git\":1, \"git:status\":2, \"git:commit\":2,\n",
    "         \"L_status_v1\":3, \"L_status_v2\":3, \"L_commit_simple\":3}\n",
    "\n",
    "tree = SimpleTree(parent, depth)\n",
    "\n",
    "# 두 리프 거리/보상\n",
    "u, v = \"L_status_v1\", \"L_status_v2\"\n",
    "d = tree.dist(u, v)                 # 같은 명령어 하위 형제 → d=2\n",
    "r = reward_inverse_distance(d)      # r = 1/(1+2) = 0.333...\n",
    "\n",
    "# 케이스별 가중(if 테이블)\n",
    "case_weights = {0: 1.0, 1: 0.7, 2: 0.4, 3: 0.2}  # 예시값\n",
    "r_w = reward_with_case_weights(d, case_weights)  # d가 0~3 사이면 해당 값 사용\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde928c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:09:36] START      | Phylo reward demo (no pytest)\n",
      "[18:09:36] CASE       | 거리 d 테스트\n",
      "[18:09:36] TEST       | dist(same leaf)              -> PASS (got=0, expect=0)\n",
      "[18:09:36] TEST       | dist(sibling leaves)         -> PASS (got=2, expect=2)\n",
      "[18:09:36] TEST       | dist(cousin leaves)          -> PASS (got=4, expect=4)\n",
      "[18:09:36] CASE       | LCA 테스트\n",
      "[18:09:36] TEST       | lca(status siblings)         -> PASS (got='git:status', expect='git:status')\n",
      "[18:09:36] TEST       | lca(status vs commit)        -> PASS (got='git', expect='git')\n",
      "[18:09:36] CASE       | 보상 r=1/(1+d) 테스트\n",
      "[18:09:36] TEST       | reward(d=0)                  -> PASS (got=1.000000000000, expect=1.000000000000)\n",
      "[18:09:36] TEST       | reward(d=2)                  -> PASS (got=0.333333333333, expect=0.333333333333)\n",
      "[18:09:36] TEST       | reward(d=4)                  -> PASS (got=0.200000000000, expect=0.200000000000)\n",
      "[18:09:36] CASE       | 케이스별 상수 보상(테이블)\n",
      "[18:09:36] TEST       | case-weight(d=2)             -> PASS (got=0.400000000000, expect=0.400000000000)\n",
      "[18:09:36] TEST       | case-weight(d=4)             -> PASS (got=0.200000000000, expect=0.200000000000)\n",
      "[18:09:36] SUMMARY    | d=0 -> r=1.000 | d=2 -> r=0.333 (or 0.400 by table) | d=4 -> r=0.200\n",
      "[18:09:36] DONE       | All checks complete.\n"
     ]
    }
   ],
   "source": [
    "# phylo_reward_demo.py\n",
    "from typing import Dict, Optional, Tuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "Node = str\n",
    "\n",
    "# -----------------------------\n",
    "# 간단 트리: LCA/거리/보상\n",
    "# -----------------------------\n",
    "class SimpleTree:\n",
    "    def __init__(self, parent: Dict[Node, Optional[Node]], depth: Dict[Node, int]):\n",
    "        self.parent = parent\n",
    "        self.depth = depth\n",
    "\n",
    "    def _align_depth(self, u: Node, v: Node) -> Tuple[Node, Node]:\n",
    "        du, dv = self.depth[u], self.depth[v]\n",
    "        while du > dv:\n",
    "            u = self.parent[u]; du -= 1\n",
    "        while dv > du:\n",
    "            v = self.parent[v]; dv -= 1\n",
    "        return u, v\n",
    "\n",
    "    def lca(self, u: Node, v: Node) -> Node:\n",
    "        u, v = self._align_depth(u, v)\n",
    "        path = []\n",
    "        while u != v:\n",
    "            path.append((u, v))\n",
    "            u = self.parent[u]\n",
    "            v = self.parent[v]\n",
    "        return u\n",
    "\n",
    "    def dist(self, u: Node, v: Node) -> int:\n",
    "        a = self.lca(u, v)\n",
    "        return (self.depth[u] - self.depth[a]) + (self.depth[v] - self.depth[a])\n",
    "\n",
    "def reward_inverse_distance(d: int) -> float:\n",
    "    return 1.0 / (1 + d)\n",
    "\n",
    "def reward_with_case_weights(d: int, weights: Dict[int, float]) -> float:\n",
    "    return weights.get(d, 1.0 / (1 + d))\n",
    "\n",
    "# -----------------------------\n",
    "# 로거(깔끔한 콘솔 출력)\n",
    "# -----------------------------\n",
    "def log(section: str, msg: str = \"\"):\n",
    "    ts = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{ts}] {section:<10} | {msg}\")\n",
    "\n",
    "def assert_eq(name: str, got, expect):\n",
    "    ok = got == expect\n",
    "    status = \"PASS\" if ok else \"FAIL\"\n",
    "    log(\"TEST\", f\"{name:<28} -> {status} (got={got!r}, expect={expect!r})\")\n",
    "    return ok\n",
    "\n",
    "def assert_close(name: str, got: float, expect: float, tol: float = 1e-9):\n",
    "    ok = abs(got - expect) <= tol * max(1.0, abs(expect))\n",
    "    status = \"PASS\" if ok else \"FAIL\"\n",
    "    log(\"TEST\", f\"{name:<28} -> {status} (got={got:.12f}, expect={expect:.12f})\")\n",
    "    return ok\n",
    "\n",
    "# -----------------------------\n",
    "# 데모 트리(루트→프로그램→명령어→리프)\n",
    "# 깊이: R(0) -> P(1) -> C(2) -> L(3)\n",
    "# -----------------------------\n",
    "def build_demo_tree() -> SimpleTree:\n",
    "    parent = {\n",
    "        \"R\": None,\n",
    "        \"git\": \"R\",\n",
    "        \"git:status\": \"git\",\n",
    "        \"git:commit\": \"git\",\n",
    "        # 리프\n",
    "        \"L_status_v1\": \"git:status\",\n",
    "        \"L_status_v2\": \"git:status\",\n",
    "        \"L_commit_simple\": \"git:commit\",\n",
    "    }\n",
    "    depth = {\n",
    "        \"R\": 0,\n",
    "        \"git\": 1,\n",
    "        \"git:status\": 2,\n",
    "        \"git:commit\": 2,\n",
    "        \"L_status_v1\": 3,\n",
    "        \"L_status_v2\": 3,\n",
    "        \"L_commit_simple\": 3,\n",
    "    }\n",
    "    return SimpleTree(parent, depth)\n",
    "\n",
    "# -----------------------------\n",
    "# 시나리오 실행 & 로깅\n",
    "# -----------------------------\n",
    "def run_demo():\n",
    "    log(\"START\", \"Phylo reward demo (no pytest)\")\n",
    "    tree = build_demo_tree()\n",
    "\n",
    "    # 거리 테스트\n",
    "    log(\"CASE\", \"거리 d 테스트\")\n",
    "    d00 = tree.dist(\"L_status_v1\", \"L_status_v1\")  # 0\n",
    "    d02 = tree.dist(\"L_status_v1\", \"L_status_v2\")  # 2 (형제)\n",
    "    d04 = tree.dist(\"L_status_v1\", \"L_commit_simple\")  # 4 (사촌, LCA=git)\n",
    "\n",
    "    assert_eq(\"dist(same leaf)\", d00, 0)\n",
    "    assert_eq(\"dist(sibling leaves)\", d02, 2)\n",
    "    assert_eq(\"dist(cousin leaves)\", d04, 4)\n",
    "\n",
    "    # LCA 테스트\n",
    "    log(\"CASE\", \"LCA 테스트\")\n",
    "    a1 = tree.lca(\"L_status_v1\", \"L_status_v2\")\n",
    "    a2 = tree.lca(\"L_status_v1\", \"L_commit_simple\")\n",
    "    assert_eq(\"lca(status siblings)\", a1, \"git:status\")\n",
    "    assert_eq(\"lca(status vs commit)\", a2, \"git\")\n",
    "\n",
    "    # 보상: 역수형\n",
    "    log(\"CASE\", \"보상 r=1/(1+d) 테스트\")\n",
    "    r0 = reward_inverse_distance(d00)  # 1.0\n",
    "    r2 = reward_inverse_distance(d02)  # 1/3\n",
    "    r4 = reward_inverse_distance(d04)  # 1/5\n",
    "    assert_close(\"reward(d=0)\", r0, 1.0)\n",
    "    assert_close(\"reward(d=2)\", r2, 1.0/3.0)\n",
    "    assert_close(\"reward(d=4)\", r4, 1.0/5.0)\n",
    "\n",
    "    # 보상: 케이스별 상수(weight 테이블)\n",
    "    log(\"CASE\", \"케이스별 상수 보상(테이블)\")\n",
    "    weights = {0: 1.0, 2: 0.40, 3: 0.22}  # 예시: d=2는 0.40으로 덮어쓰기\n",
    "    rw2 = reward_with_case_weights(d02, weights)\n",
    "    rw4 = reward_with_case_weights(d04, weights)  # 테이블에 없으니 역수형 폴백(0.2)\n",
    "    assert_close(\"case-weight(d=2)\", rw2, 0.40)\n",
    "    assert_close(\"case-weight(d=4)\", rw4, 1.0/5.0)\n",
    "\n",
    "    # 요약 출력\n",
    "    log(\"SUMMARY\", f\"d=0 -> r={r0:.3f} | d=2 -> r={r2:.3f} (or {rw2:.3f} by table) | d=4 -> r={r4:.3f}\")\n",
    "\n",
    "    log(\"DONE\", \"All checks complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ded50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#뭐가 어떻게 될지 몰라 일단 함수 기틀만 만들어 보았음.\n",
    "'''우선, 호기심 보상 함수를 두가지 버전으로 만들어보앗는데,\n",
    "첫번째로는 은세가 말했던 스텝이 진행될 수록 기틀이 잡히니까 호기심 보상을 자동적으로 감쇠시키는 버전,\n",
    "두번째로는 후반에 가서도 호기심 보상을 감소시키지 않을 수 있게 최악의 상태일떄 음의 보상으로 패치하는 버전으로 만들었음.\n",
    "'''\n",
    "\n",
    "\n",
    "#자동 감쇠형 호기심 함수\n",
    "def curiosity_reward_decay(step, base_reward=1.0, decay_rate=0.005):\n",
    "    \"\"\"\n",
    "    호기심 보상이 스텝(step)에 따라 자연스럽게 감소하는 함수.\n",
    "    예: 초반엔 높은 보상, 후반으로 갈수록 점점 감소.\n",
    "\n",
    "    R_c(t) = base_reward * (1 - decay_rate * step)\n",
    "    (단, 0보다 작으면 0으로 제한)\n",
    "    \"\"\"\n",
    "    reward = max(base_reward * (1 - decay_rate * step), 0)\n",
    "    return reward\n",
    "# 최악의 경우에 음의 보상을 주는 호기심 함수\n",
    "def curiosity_reward_with_penalty(is_redundant=False, is_error=False, step=0,\n",
    "                                  base_reward=1.0, penalty_redundant=0.3, penalty_error=0.5,\n",
    "                                  decay_rate=0.003):\n",
    "    \"\"\"\n",
    "    스텝 수에 따른 호기심 보상 감소 + 잘못된 행동에 대한 음의 보상 적용.\n",
    "\n",
    "    Parameters:\n",
    "    - is_redundant: 이미 여러 번 시도된 동일한 행동 수행시 True ---\n",
    "    - is_error: 시스템 오류를 유발하는 탐색시 True >>>\n",
    "    - step: 현재 스텝 수 >>>\n",
    "    - base_reward: 기본 호기심 보상 >>>\n",
    "    - penalty_redundant: 반복 행동에 부여할 음의 보상 강도\n",
    "    - penalty_error: 심각한 오류 행위에 부여할 음의 보상 강도\n",
    "    - decay_rate: 스텝에 따른 감쇠 비율\n",
    "    \"\"\"\n",
    "    reward = max(base_reward * (1 - decay_rate * step), 0)\n",
    "    if is_redundant:\n",
    "        reward -= penalty_redundant\n",
    "    if is_error:\n",
    "        reward -= penalty_error\n",
    "    return max(reward, -1.0)\n",
    "\n",
    "\n",
    "\n",
    "# 새로운 행동 -> 보상\n",
    "\n",
    "# 시간 감쇠\n",
    "\n",
    "# 연속 반복 페널티\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533eba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def flag_reward(flag_str, method, known_flags, found_paths):\n",
    "    \"\"\"\n",
    "    FLAG 보상 함수 (기틀 버전)\n",
    "    -----------------------------------------------------\n",
    "    플래그 문자열을 발견했을 때 보상을 계산하는 함수.\n",
    "\n",
    "    매개변수 설명:\n",
    "    - flag_str (str): 코드 속에 끼워져 있는 플래그 원문 문자열\n",
    "    - method (str): 해당 플래그를 찾은 탐색 경로나 행동(명령) 이름\n",
    "    - known_flags (dict): 사전 등록된 정답 플래그 해시 목록 {flag_name: sha256_hex}\n",
    "    - found_paths (dict): 이미 찾은 플래그 및 경로 기록 {flag_name: [methods]}\n",
    "    -----------------------------------------------------\n",
    "    반환값:\n",
    "    - reward (float): 계산된 최종 보상값 (단일 스칼라)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 기본 보상 정책(수정 가능) ---\n",
    "    BASE_REWARD = 5.0         # 올바른 플래그를 찾았을 때의 기본 보상\n",
    "    NEW_PATH_BONUS = 2.0      # 같은 플래그를 '새로운 경로'로 찾았을 때 보너스\n",
    "\n",
    "    reward = 0.0  # 최종 보상 초기화\n",
    "\n",
    "    # 1) 플래그 해시 계산 (sha256 권장: 실행 간 일관성 확보)\n",
    "    flag_hash = hashlib.sha256(flag_str.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    # ================================================================\n",
    "    # [PARSE_HOOK - START]\n",
    "\n",
    "    #\n",
    "    # 아래 변수들은 '존재하면 보상 가중에 반영'되도록 설계됨.\n",
    "    parsed_reward = None   # 예: 문자열에서 읽은 가산 보상(float). 없으면 None 유지\n",
    "    parsed_flag_id = None  # 예: 플래그를 특정하는 ID가 있다면 사용(str)\n",
    "    parsed_tags = None     # 예: ['web','sql'] 같은 태그 리스트. 정책에 활용 가능\n",
    "   \n",
    "    # ================================================================\n",
    "\n",
    "    # 2) known_flags와 해시 매칭\n",
    "    matched_flag_name = None\n",
    "    for flag_name, valid_hash in known_flags.items():\n",
    "        if flag_hash == valid_hash:\n",
    "            matched_flag_name = flag_name\n",
    "            break\n",
    "\n",
    "    # 3) 매칭 실패 시 보상 0 반환\n",
    "    if matched_flag_name is None:\n",
    "        return 0.0\n",
    "\n",
    "    # 4) 경로 중복 여부 확인 및 보상 결정\n",
    "    prior_methods = found_paths.get(matched_flag_name, [])\n",
    "\n",
    "    if method not in prior_methods:\n",
    "        # 새로운 경로에서 같은 플래그를 처음 발견\n",
    "        reward = BASE_REWARD + NEW_PATH_BONUS\n",
    "        found_paths.setdefault(matched_flag_name, []).append(method)\n",
    "    else:\n",
    "        # 동일 경로로 이미 발견한 적 있음 → 기본 보상만\n",
    "        reward = BASE_REWARD\n",
    "\n",
    "    # 5) (선택) 파싱된 보상 parsed_reward가 있으면 가산\n",
    "    #    친구가 파싱 부분에서 parsed_reward를 채우면 자동 반영됨.\n",
    "    if parsed_reward is not None:\n",
    "        # 필요 시 클리핑/스케일 조정 가능 (예: max(0, min(parsed_reward, 10)))\n",
    "        reward += float(parsed_reward)\n",
    "\n",
    "    # 6) (선택) parsed_flag_id / parsed_tags 활용 훅\n",
    "    #    여기서 정책적으로 특정 ID나 태그에 가중치를 줄 수도 있음.\n",
    "\n",
    "    # 7) 최종 스칼라 보상 반환\n",
    "    return float(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd87047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():   \n",
    "    error_reward()\n",
    "    prophecy_reward()\n",
    "    flag_reward()\n",
    "    curiosity()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy (ε-탐욕 기반)\n",
    "def policy_update(actions, Q, epsilon=0.1, alpha=0.1, last_action=None, reward=0):\n",
    "    \"\"\"\n",
    "    간단한 ε-탐욕 정책 함수 (기틀 버전)\n",
    "    - actions: 가능한 행동 리스트\n",
    "    - Q: 행동 가치 테이블 (dict)\n",
    "    - epsilon: 무작위 탐색 확률\n",
    "    - alpha: 학습률\n",
    "    - last_action: 직전 수행한 행동\n",
    "    - reward: 직전 보상\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    # Q값 업데이트 (최근 보상 반영)\n",
    "    if last_action is not None:\n",
    "        old_value = Q.get(last_action, 0)\n",
    "        Q[last_action] = old_value + alpha * (reward - old_value)\n",
    "\n",
    "    # ε-탐욕 정책으로 다음 행동 선택\n",
    "    if random.random() < epsilon:\n",
    "        next_action = random.choice(actions)\n",
    "    else:\n",
    "        next_action = max(actions, key=lambda a: Q.get(a, 0))\n",
    "\n",
    "    return next_action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dd32",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
