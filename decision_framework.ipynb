{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c2bad6",
   "metadata": {},
   "source": [
    "현진: 호기심\n",
    "원준: 플래그/폴리시\n",
    "나: 예측 기반 오차/오류\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "에피소드 기반 보상은 나중에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be955d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': -0.2, 'reward': 0.8, 'error_detected': True, 'error_type': 'hostname_resolution_error', 'suggested_fix': '대상 호스트 이름이 올바른지 확인하거나 IP 주소로 직접 입력하세요.', 'auto_fixable': True}\n"
     ]
    }
   ],
   "source": [
    "#에러기반\n",
    "import re\n",
    "\n",
    "#  오류 유형별 penalty 상수 선언\n",
    "PENALTY_DICT = {\n",
    "    'hostname_resolution_error':   -0.2,  # 호스트 이름 해석 실패\n",
    "    'device_access_error':         -0.3,  # 장치 접근 권한/불가\n",
    "    'network_unreachable':         -0.25, # 대상 네트워크 접근 불가\n",
    "    'invalid_target':              -0.15, # 입력 타겟값 부적절\n",
    "    'nmap_internal_error':         -0.4   # 내부 오류(비정상 종료 등)\n",
    "}\n",
    "MIN_REWARD = -1.0  # 최저 보상값 (절대치)\n",
    "MAX_REWARD = 1.0   # 최고 보상값\n",
    "\n",
    "def error(output_log):\n",
    "    \"\"\"\n",
    "    nmap 실행 중 발생한 오류 메시지를 분석하고,\n",
    "    자동으로 수정 가능한 경우 대응 조치를 제안하거나 보정한다.\n",
    "\n",
    "    Parameters:\n",
    "    - output_log (str): nmap 실행 결과 로그 문자열\n",
    "\n",
    "    Returns:\n",
    "    - dict: {\n",
    "        'error_detected': bool,      # 오류 존재 여부\n",
    "        'error_type': str,           # 오류 유형 식별(키)\n",
    "        'suggested_fix': str | None, # 제안된 수정 또는 재시도 방안\n",
    "        'auto_fixable': bool         # 자동 수정 가능 여부\n",
    "      }\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'error_detected': False,\n",
    "        'error_type': None,\n",
    "        'suggested_fix': None,\n",
    "        'auto_fixable': False\n",
    "    }\n",
    "    log = output_log.lower()\n",
    "\n",
    "    # 호스트 이름 해석 실패\n",
    "    if re.search(r\"(unable to resolve hostname|dns resolution failed|error resolving name)\", log):\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'hostname_resolution_error',\n",
    "            'suggested_fix': '대상 호스트 이름이 올바른지 확인하거나 IP 주소로 직접 입력하세요.',\n",
    "            'auto_fixable': True\n",
    "        })\n",
    "    #  장치 접근 불가\n",
    "    elif \"failed to open device\" in log:\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'device_access_error',\n",
    "            'suggested_fix': '루트 권한으로 다시 실행하거나 네트워크 인터페이스 이름을 명시하세요. (예: nmap -e eth0 ...)',\n",
    "            'auto_fixable': False\n",
    "        })\n",
    "    #  대상 접근 불가\n",
    "    elif \"no route to host\" in log:\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'network_unreachable',\n",
    "            'suggested_fix': '대상 네트워크 연결 상태를 점검하고, 방화벽 또는 VPN 설정을 확인하세요.',\n",
    "            'auto_fixable': False\n",
    "        })\n",
    "    #  유효하지 않은 대상\n",
    "    elif \"not a valid target\" in log:\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'invalid_target',\n",
    "            'suggested_fix': '입력한 대상 형식(IP, CIDR 등)을 다시 확인하세요.',\n",
    "            'auto_fixable': True\n",
    "        })\n",
    "    #  nmap 내부 오류 (일반적 종료)\n",
    "    elif re.search(r\"(nmap error|quitting)\", log):\n",
    "        result.update({\n",
    "            'error_detected': True,\n",
    "            'error_type': 'nmap_internal_error',\n",
    "            'suggested_fix': 'nmap 명령 구문과 옵션을 다시 확인하거나 -d(디버그 모드)로 재실행하세요.',\n",
    "            'auto_fixable': False\n",
    "        })\n",
    "    #  오류 없음\n",
    "    else:\n",
    "        result.update({\n",
    "            'error_detected': False,\n",
    "            'error_type': None,\n",
    "            'suggested_fix': None,\n",
    "            'auto_fixable': False\n",
    "        })\n",
    "    return result\n",
    "\n",
    "def calc_penalty(log):\n",
    "    \"\"\"\n",
    "    error() 함수로 식별된 오류 유형에 따라 해당 penalty만큼\n",
    "    보상을 감소시키는 단일 책임 함수. 적용 penalty는 최저/최고 보상값 범위 내로 제한.\n",
    "    \"\"\"\n",
    "    err = error(log)  # 오류 분석 실행\n",
    "    # 오류 유형이 발견되면 PENALTY_DICT 값을 적용, 아니면 0(감점 없음)\n",
    "    penalty = PENALTY_DICT.get(err['error_type'], 0.0)\n",
    "    # 총 보상(예: 1.0에서 penalty만큼 빼기, 실제 시스템에서는 누적 보상/스텝별 반영)\n",
    "    reward = max(MIN_REWARD, min(MAX_REWARD, 1.0 + penalty))\n",
    "    # 상세 결과표와 계산된 보상 함께 반환\n",
    "    return {'penalty': penalty, 'reward': reward, **err}\n",
    "\n",
    "# 예시 사용\n",
    "log1 = \"Nmap scan report: Unable to resolve hostname example.local\"\n",
    "print(calc_penalty(log1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde928c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:11] START      | Phylo reward demo (no pytest)\n",
      "[18:16:11] CASE       | 거리 d 테스트\n",
      "[18:16:11] TEST       | dist(same leaf)              -> PASS (got=0, expect=0)\n",
      "[18:16:11] TEST       | dist(sibling leaves)         -> PASS (got=2, expect=2)\n",
      "[18:16:11] TEST       | dist(cousin leaves)          -> PASS (got=4, expect=4)\n",
      "[18:16:11] CASE       | LCA 테스트\n",
      "[18:16:11] TEST       | lca(status siblings)         -> PASS (got='git:status', expect='git:status')\n",
      "[18:16:11] TEST       | lca(status vs commit)        -> PASS (got='git', expect='git')\n",
      "[18:16:11] CASE       | 보상 r=1/(1+d) 테스트\n",
      "[18:16:11] TEST       | reward(d=0)                  -> PASS (got=1.000000000000, expect=1.000000000000)\n",
      "[18:16:11] TEST       | reward(d=2)                  -> PASS (got=0.333333333333, expect=0.333333333333)\n",
      "[18:16:11] TEST       | reward(d=4)                  -> PASS (got=0.200000000000, expect=0.200000000000)\n",
      "[18:16:11] CASE       | 케이스별 상수 보상(테이블)\n",
      "[18:16:11] TEST       | case-weight(d=2)             -> PASS (got=0.400000000000, expect=0.400000000000)\n",
      "[18:16:11] TEST       | case-weight(d=4)             -> PASS (got=0.200000000000, expect=0.200000000000)\n",
      "[18:16:11] SUMMARY    | d=0 -> r=1.000 | d=2 -> r=0.333 (or 0.400 by table) | d=4 -> r=0.200\n",
      "[18:16:11] DONE       | All checks complete.\n"
     ]
    }
   ],
   "source": [
    "# phylo_reward_demo.py\n",
    "from typing import Dict, Optional, Tuple\n",
    "import math\n",
    "import time\n",
    "\n",
    "Node = str\n",
    "\n",
    "# -----------------------------\n",
    "# 간단 트리: LCA/거리/보상\n",
    "# -----------------------------\n",
    "class SimpleTree:\n",
    "    def __init__(self, parent: Dict[Node, Optional[Node]], depth: Dict[Node, int]):\n",
    "        self.parent = parent\n",
    "        self.depth = depth\n",
    "\n",
    "    def _align_depth(self, u: Node, v: Node) -> Tuple[Node, Node]:\n",
    "        du, dv = self.depth[u], self.depth[v]\n",
    "        while du > dv:\n",
    "            u = self.parent[u]; du -= 1\n",
    "        while dv > du:\n",
    "            v = self.parent[v]; dv -= 1\n",
    "        return u, v\n",
    "\n",
    "    def lca(self, u: Node, v: Node) -> Node:\n",
    "        u, v = self._align_depth(u, v)\n",
    "        path = []\n",
    "        while u != v:\n",
    "            path.append((u, v))\n",
    "            u = self.parent[u]\n",
    "            v = self.parent[v]\n",
    "        return u\n",
    "\n",
    "    def dist(self, u: Node, v: Node) -> int:\n",
    "        a = self.lca(u, v)\n",
    "        return (self.depth[u] - self.depth[a]) + (self.depth[v] - self.depth[a])\n",
    "\n",
    "def reward_inverse_distance(d: int) -> float:\n",
    "    return 1.0 / (1 + d)\n",
    "\n",
    "def reward_with_case_weights(d: int, weights: Dict[int, float]) -> float:\n",
    "    return weights.get(d, 1.0 / (1 + d))\n",
    "\n",
    "# -----------------------------\n",
    "# 로거(깔끔한 콘솔 출력)\n",
    "# -----------------------------\n",
    "def log(section: str, msg: str = \"\"):\n",
    "    ts = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{ts}] {section:<10} | {msg}\")\n",
    "\n",
    "def assert_eq(name: str, got, expect):\n",
    "    ok = got == expect\n",
    "    status = \"PASS\" if ok else \"FAIL\"\n",
    "    log(\"TEST\", f\"{name:<28} -> {status} (got={got!r}, expect={expect!r})\")\n",
    "    return ok\n",
    "\n",
    "def assert_close(name: str, got: float, expect: float, tol: float = 1e-9):\n",
    "    ok = abs(got - expect) <= tol * max(1.0, abs(expect))\n",
    "    status = \"PASS\" if ok else \"FAIL\"\n",
    "    log(\"TEST\", f\"{name:<28} -> {status} (got={got:.12f}, expect={expect:.12f})\")\n",
    "    return ok\n",
    "\n",
    "# -----------------------------\n",
    "# 데모 트리(루트→프로그램→명령어→리프)\n",
    "# 깊이: R(0) -> P(1) -> C(2) -> L(3)\n",
    "# -----------------------------\n",
    "def build_demo_tree() -> SimpleTree:\n",
    "    parent = {\n",
    "        \"R\": None,\n",
    "        \"git\": \"R\",\n",
    "        \"git:status\": \"git\",\n",
    "        \"git:commit\": \"git\",\n",
    "        # 리프\n",
    "        \"L_status_v1\": \"git:status\",\n",
    "        \"L_status_v2\": \"git:status\",\n",
    "        \"L_commit_simple\": \"git:commit\",\n",
    "    }\n",
    "    depth = {\n",
    "        \"R\": 0,\n",
    "        \"git\": 1,\n",
    "        \"git:status\": 2,\n",
    "        \"git:commit\": 2,\n",
    "        \"L_status_v1\": 3,\n",
    "        \"L_status_v2\": 3,\n",
    "        \"L_commit_simple\": 3,\n",
    "    }\n",
    "    return SimpleTree(parent, depth)\n",
    "\n",
    "# -----------------------------\n",
    "# 시나리오 실행 & 로깅\n",
    "# -----------------------------\n",
    "def run_demo():\n",
    "    log(\"START\", \"Phylo reward demo (no pytest)\")\n",
    "    tree = build_demo_tree()\n",
    "\n",
    "    # 거리 테스트\n",
    "    log(\"CASE\", \"거리 d 테스트\")\n",
    "    d00 = tree.dist(\"L_status_v1\", \"L_status_v1\")  # 0\n",
    "    d02 = tree.dist(\"L_status_v1\", \"L_status_v2\")  # 2 (형제)\n",
    "    d04 = tree.dist(\"L_status_v1\", \"L_commit_simple\")  # 4 (사촌, LCA=git)\n",
    "\n",
    "    assert_eq(\"dist(same leaf)\", d00, 0)\n",
    "    assert_eq(\"dist(sibling leaves)\", d02, 2)\n",
    "    assert_eq(\"dist(cousin leaves)\", d04, 4)\n",
    "\n",
    "    # LCA 테스트\n",
    "    log(\"CASE\", \"LCA 테스트\")\n",
    "    a1 = tree.lca(\"L_status_v1\", \"L_status_v2\")\n",
    "    a2 = tree.lca(\"L_status_v1\", \"L_commit_simple\")\n",
    "    assert_eq(\"lca(status siblings)\", a1, \"git:status\")\n",
    "    assert_eq(\"lca(status vs commit)\", a2, \"git\")\n",
    "\n",
    "    # 보상: 역수형\n",
    "    log(\"CASE\", \"보상 r=1/(1+d) 테스트\")\n",
    "    r0 = reward_inverse_distance(d00)  # 1.0\n",
    "    r2 = reward_inverse_distance(d02)  # 1/3\n",
    "    r4 = reward_inverse_distance(d04)  # 1/5\n",
    "    assert_close(\"reward(d=0)\", r0, 1.0)\n",
    "    assert_close(\"reward(d=2)\", r2, 1.0/3.0)\n",
    "    assert_close(\"reward(d=4)\", r4, 1.0/5.0)\n",
    "\n",
    "    # 보상: 케이스별 상수(weight 테이블)\n",
    "    log(\"CASE\", \"케이스별 상수 보상(테이블)\")\n",
    "    weights = {0: 1.0, 2: 0.40, 3: 0.22}  # 예시: d=2는 0.40으로 덮어쓰기\n",
    "    rw2 = reward_with_case_weights(d02, weights)\n",
    "    rw4 = reward_with_case_weights(d04, weights)  # 테이블에 없으니 역수형 폴백(0.2)\n",
    "    assert_close(\"case-weight(d=2)\", rw2, 0.40)\n",
    "    assert_close(\"case-weight(d=4)\", rw4, 1.0/5.0)\n",
    "\n",
    "    # 요약 출력\n",
    "    log(\"SUMMARY\", f\"d=0 -> r={r0:.3f} | d=2 -> r={r2:.3f} (or {rw2:.3f} by table) | d=4 -> r={r4:.3f}\")\n",
    "\n",
    "    log(\"DONE\", \"All checks complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ded50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\n",
    "# ========================================================\n",
    "# 호기심 보상 시스템 설정값 (전역 상수)\n",
    "# ========================================================\n",
    "\n",
    "# 보상 스케일 설정\n",
    "CURIOSITY_BASE_REWARD = 1.0          # 호기심 기본 보상\n",
    "CURIOSITY_MIN_REWARD = -1.0          # 호기심 최소 보상 (음수 제한)\n",
    "CURIOSITY_MAX_REWARD = 1.0           # 호기심 최대 보상\n",
    "\n",
    "# 감쇠 설정\n",
    "DECAY_STRENGTH = 0.1                 # 로그 감쇠 강도\n",
    "\n",
    "# 패널티 기준\n",
    "MAX_ALLOWED_REPEATS = 5              # 반복 허용 횟수\n",
    "MIN_INFO_GAIN_THRESHOLD = 0.005      # 정보 증가 최소 임계값\n",
    "\n",
    "# 패널티 강도\n",
    "PENALTY_REDUNDANT = 0.2              # 반복 행동 패널티\n",
    "PENALTY_ERROR = 0.3                  # 일반 오류 패널티\n",
    "PENALTY_CRITICAL = 0.7               # 심각한 오류 패널티\n",
    "PENALTY_ACCESS_DENIED = 0.15         # 접근 제한 패널티\n",
    "PENALTY_INFO_DEFICIT_MULTIPLIER = 15 # 정보 부족 패널티 배수\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# 1. 자동 감쇠형 호기심 보상 함수 (로그 감쇠)\n",
    "# ========================================================\n",
    "\n",
    "def curiosity_reward_decay(step):  # 통합필요: step\n",
    "    \"\"\"\n",
    "    호기심 보상을 로그 함수 형태로 감쇠시키는 함수.\n",
    "    \n",
    "    후반부에도 완전히 0이 되지 않고 일정 수준 유지.\n",
    "    수식: R_c(t) = BASE / (1 + DECAY_STRENGTH * log(1 + step))\n",
    "    \n",
    "    Parameters:\n",
    "    - step: 현재 스텝 수 (전체 학습 진행도)  # 통합필요\n",
    "    \n",
    "    Returns:\n",
    "    - 감쇠된 호기심 보상  # 통합필요\n",
    "    \"\"\"\n",
    "    reward = CURIOSITY_BASE_REWARD / (1 + DECAY_STRENGTH * math.log1p(step))\n",
    "    \n",
    "    # 보상 범위 제한\n",
    "    reward = max(CURIOSITY_MIN_REWARD, min(CURIOSITY_MAX_REWARD, reward))\n",
    "    \n",
    "    return reward  # 통합필요\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# 2. 음의 보상 포함형 호기심 보상 함수\n",
    "# ========================================================\n",
    "\n",
    "def curiosity_reward_with_penalty(is_redundant, is_error, is_critical, step):  # 통합필요: step\n",
    "    \"\"\"\n",
    "    로그 감쇠 + 조건부 음의 보상을 결합한 호기심 보상 함수.\n",
    "    \n",
    "    Parameters:\n",
    "    - is_redundant: 과도한 반복 여부  # 통합필요\n",
    "    - is_error: 일반 오류 발생 여부  # 통합필요\n",
    "    - is_critical: 심각한 시스템 오류 발생 여부  # 통합필요\n",
    "    - step: 현재 스텝 수  # 통합필요\n",
    "    \n",
    "    Returns:\n",
    "    - 최종 호기심 보상  # 통합필요\n",
    "    \"\"\"\n",
    "    # 기본 로그 감쇠 적용\n",
    "    reward = CURIOSITY_BASE_REWARD / (1 + DECAY_STRENGTH * math.log1p(step))\n",
    "    \n",
    "    # 조건부 패널티 적용\n",
    "    if is_redundant:\n",
    "        reward -= PENALTY_REDUNDANT\n",
    "    if is_error:\n",
    "        reward -= PENALTY_ERROR\n",
    "    if is_critical:\n",
    "        reward -= PENALTY_CRITICAL\n",
    "    \n",
    "    # 보상 범위 제한\n",
    "    reward = max(CURIOSITY_MIN_REWARD, min(CURIOSITY_MAX_REWARD, reward))\n",
    "    \n",
    "    return reward  # 통합필요\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# 3. 음의 보상 조건 판별 함수\n",
    "# ========================================================\n",
    "\n",
    "def check_negative_reward_conditions(\n",
    "        action_log,         # 통합필요: 지금까지 수행한 행동 리스트\n",
    "        current_action,     # 통합필요: 현재 수행한 행동\n",
    "        output_log,         # 통합필요: 툴 실행 후 출력 로그\n",
    "        knowledge_gain,     # 통합필요: 이번 행동으로 얻은 정보량 (0~1)\n",
    "        error_keywords=None,\n",
    "        critical_error_keywords=None,\n",
    "        system_keywords=None):\n",
    "    \"\"\"\n",
    "    음의 보상 부여 조건을 감지하는 함수.\n",
    "    \n",
    "    Parameters:\n",
    "    - action_log: 이전까지 수행된 모든 행동 리스트  # 통합필요\n",
    "    - current_action: 현재 시도한 행동  # 통합필요\n",
    "    - output_log: 명령 실행 후 출력된 로그 텍스트  # 통합필요\n",
    "    - knowledge_gain: 이번 행동을 통해 얻은 새로운 정보의 양 (0~1 스케일)  # 통합필요\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {\n",
    "        'redundant': bool,      # 과도한 반복 여부  # 통합필요\n",
    "        'error': bool,          # 일반 오류 발생 여부  # 통합필요\n",
    "        'critical': bool,       # 심각한 오류 여부  # 통합필요\n",
    "        'inefficient': bool,    # 비효율적 탐색 여부  # 통합필요\n",
    "        'penalty_score': float  # 총 패널티 점수 (음수)  # 통합필요\n",
    "      }\n",
    "    \"\"\"\n",
    "    \n",
    "    # 기본 키워드 설정\n",
    "    if error_keywords is None:\n",
    "        error_keywords = [\n",
    "            \"error\", \"failed\", \"exception\", \"denied\", \"invalid\", \n",
    "            \"timeout\", \"refused\", \"not found\"\n",
    "        ]\n",
    "    \n",
    "    if critical_error_keywords is None:\n",
    "        critical_error_keywords = [\n",
    "            \"segmentation fault\", \"core dumped\", \"crash\", \"fatal\", \n",
    "            \"terminated\", \"killed\", \"panic\"\n",
    "        ]\n",
    "    \n",
    "    if system_keywords is None:\n",
    "        system_keywords = [\n",
    "            \"unauthorized\", \"access denied\", \"permission\", \"firewall\",\n",
    "            \"blocked\", \"forbidden\"\n",
    "        ]\n",
    "    \n",
    "    # 결과 초기화\n",
    "    result = {\n",
    "        'redundant': False,      # 통합필요\n",
    "        'error': False,          # 통합필요\n",
    "        'critical': False,       # 통합필요\n",
    "        'inefficient': False,    # 통합필요\n",
    "        'penalty_score': 0.0     # 통합필요\n",
    "    }\n",
    "    \n",
    "    # (1) 반복 행동 감지\n",
    "    repeat_count = action_log.count(current_action)  # 통합필요: action_log, current_action\n",
    "    if repeat_count >= MAX_ALLOWED_REPEATS:\n",
    "        result['redundant'] = True  # 통합필요\n",
    "        excess_repeats = repeat_count - MAX_ALLOWED_REPEATS + 1\n",
    "        result['penalty_score'] -= PENALTY_REDUNDANT * excess_repeats  # 통합필요\n",
    "    \n",
    "    # (2) 일반 오류 감지\n",
    "    error_found = any(\n",
    "        re.search(rf\"\\b{kw}\\b\", output_log, re.IGNORECASE)  # 통합필요: output_log\n",
    "        for kw in error_keywords\n",
    "    )\n",
    "    if error_found:\n",
    "        result['error'] = True  # 통합필요\n",
    "        result['penalty_score'] -= PENALTY_ERROR  # 통합필요\n",
    "    \n",
    "    # (3) 심각한 시스템 오류 감지\n",
    "    critical_found = any(\n",
    "        re.search(rf\"\\b{kw}\\b\", output_log, re.IGNORECASE)  # 통합필요: output_log\n",
    "        for kw in critical_error_keywords\n",
    "    )\n",
    "    if critical_found:\n",
    "        result['critical'] = True  # 통합필요\n",
    "        result['penalty_score'] -= PENALTY_CRITICAL  # 통합필요\n",
    "    \n",
    "    # (4) 접근 제한 감지\n",
    "    system_block = any(\n",
    "        re.search(rf\"\\b{kw}\\b\", output_log, re.IGNORECASE)  # 통합필요: output_log\n",
    "        for kw in system_keywords\n",
    "    )\n",
    "    if system_block:\n",
    "        result['error'] = True  # 통합필요\n",
    "        result['penalty_score'] -= PENALTY_ACCESS_DENIED  # 통합필요\n",
    "    \n",
    "    # (5) 비효율적 탐색 감지\n",
    "    if knowledge_gain < MIN_INFO_GAIN_THRESHOLD:  # 통합필요: knowledge_gain\n",
    "        result['inefficient'] = True  # 통합필요\n",
    "        info_deficit = MIN_INFO_GAIN_THRESHOLD - knowledge_gain\n",
    "        result['penalty_score'] -= info_deficit * PENALTY_INFO_DEFICIT_MULTIPLIER  # 통합필요\n",
    "    \n",
    "    # (6) 패널티 점수 범위 제한\n",
    "    result['penalty_score'] = max(CURIOSITY_MIN_REWARD, result['penalty_score'])  # 통합필요\n",
    "    \n",
    "    return result  # 통합필요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533eba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import hmac\n",
    "\n",
    "def flag_reward(flag_str, known_flags, big_reward=1000.0):\n",
    "    \"\"\"\n",
    "    플래그 정답 여부에 따른 스칼라 보상 계산 함수 (순수 함수)\n",
    "    \n",
    "    파라미터:\n",
    "    - flag_str     : (str) 제출/발견된 플래그 원문\n",
    "    - known_flags  : (dict) {\"FLAG\": \"<sha256_hex>\"}  (정답 해시 딕셔너리)\n",
    "    - big_reward   : (float) 정답시 지급 보상 크기 (기본 1000.0)\n",
    "    \n",
    "    반환값:\n",
    "    - reward       : (float) 정답 시 big_reward, 아니면 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    # 입력 검증\n",
    "    if not isinstance(flag_str, str):\n",
    "        return 0.0\n",
    "\n",
    "    # 제출 플래그 해시 계산\n",
    "    submitted_hash = hashlib.sha256(flag_str.strip().encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    # 등록된 정답 해시 확인\n",
    "    try:\n",
    "        expected_key, expected_hash = next(iter(known_flags.items()))\n",
    "    except StopIteration:\n",
    "        return 0.0\n",
    "\n",
    "    # 안전 비교\n",
    "    is_correct = hmac.compare_digest(submitted_hash, expected_hash)\n",
    "\n",
    "    # 보상 산출 (오직 정답만 지급, 외부 상태 미변경)\n",
    "    reward = float(big_reward) if is_correct else 0.0\n",
    "\n",
    "    # [PARSE_HOOK]  \n",
    "    # 필요하면 여기서 플래그 내 추가 메타데이터(REWARD= 등) 파싱 확장\n",
    "    # 예시:\n",
    "    #   parsed_reward = parse_reward_from_string(flag_str)\n",
    "    #   if is_correct and parsed_reward is not None:\n",
    "    #       reward += float(parsed_reward)\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd87047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():   \n",
    "    error_reward()\n",
    "    prophecy_reward()\n",
    "    flag_reward()\n",
    "    curiosity()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the policy manager code to a file and execute it so it runs inside this environment.\n",
    "# The schema file is already uploaded at /mnt/data/tools_0.2.1.json.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Any, Optional, Sequence, Tuple\n",
    "import json, math, random, os, sys\n",
    "\n",
    "# ==========================\n",
    "# Constants (top of file)\n",
    "# ==========================\n",
    "SCHEMA_PATH: str = \"/mnt/data/tools_0.2.1.json\"  # Uploaded schema file\n",
    "ALPHA: float = 0.10            # learning rate\n",
    "EPS_ACTION: float = 0.05       # epsilon for stage 1 (actions)\n",
    "EPS_FLAT: float = 0.05         # epsilon for stage 2 (flats)\n",
    "INIT_Q_ACTION: float = 0.0     # initial Q value for actions\n",
    "INIT_Q_FLAT: float = 0.0       # initial Q value for flats\n",
    "RNG_SEED: int = 42             # RNG seed for reproducibility\n",
    "\n",
    "random.seed(RNG_SEED)\n",
    "\n",
    "def sigmoid(x: float) -> float:\n",
    "    if x >= 0:\n",
    "        z = math.exp(-x)\n",
    "        return 1.0 / (1.0 + z)\n",
    "    else:\n",
    "        z = math.exp(x)\n",
    "        return z / (1.0 + z)\n",
    "\n",
    "def normalize(probs: Sequence[float]) -> List[float]:\n",
    "    s = float(sum(probs))\n",
    "    if s <= 0.0:\n",
    "        n = len(probs)\n",
    "        return [1.0 / n for _ in range(n)] if n > 0 else []\n",
    "    return [p / s for p in probs]\n",
    "\n",
    "def to_probs_from_Q(q_values: Sequence[float]) -> List[float]:\n",
    "    s = [sigmoid(v) for v in q_values]\n",
    "    return normalize(s)\n",
    "\n",
    "def mix_with_epsilon(p: Sequence[float], eps: float) -> List[float]:\n",
    "    n = len(p)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    u = 1.0 / n\n",
    "    return [(1.0 - eps) * pi + eps * u for pi in p]\n",
    "\n",
    "def sample_index(p: Sequence[float]) -> int:\n",
    "    r = random.random()\n",
    "    acc = 0.0\n",
    "    for i, pi in enumerate(p):\n",
    "        acc += pi\n",
    "        if r <= acc:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def load_actions_from_schema(path: str) -> List[str]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Schema file not found: {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    actions = [a.get(\"name\") for a in data.get(\"actions\", []) if a.get(\"name\")]\n",
    "    if not actions:\n",
    "        raise ValueError(\"No actions found in schema\")\n",
    "    return actions\n",
    "\n",
    "@dataclass\n",
    "class TwoStageTablePolicy:\n",
    "    actions: List[str]\n",
    "    alpha: float = ALPHA\n",
    "    eps_action: float = EPS_ACTION\n",
    "    eps_flat: float = EPS_FLAT\n",
    "    init_q_action: float = INIT_Q_ACTION\n",
    "    init_q_flat: float = INIT_Q_FLAT\n",
    "\n",
    "    Q_action: Dict[str, float] = field(default_factory=dict)\n",
    "    Q_flat: Dict[str, Dict[str, float]] = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        for a in self.actions:\n",
    "            self.Q_action.setdefault(a, self.init_q_action)\n",
    "            self.Q_flat.setdefault(a, {})\n",
    "\n",
    "    def action_probs(self) -> List[Tuple[str, float]]:\n",
    "        q_list = [self.Q_action[a] for a in self.actions]\n",
    "        p = to_probs_from_Q(q_list)\n",
    "        p = mix_with_epsilon(p, self.eps_action)\n",
    "        return list(zip(self.actions, p))\n",
    "\n",
    "    def choose_action(self) -> str:\n",
    "        pairs = self.action_probs()\n",
    "        names, probs = zip(*pairs)\n",
    "        idx = sample_index(probs)\n",
    "        return names[idx]\n",
    "\n",
    "    def flat_probs(self, action: str, flats: Sequence[str]) -> List[Tuple[str, float]]:\n",
    "        for x in flats:\n",
    "            self.Q_flat[action].setdefault(x, self.init_q_flat)\n",
    "        q_list = [self.Q_flat[action][x] for x in flats]\n",
    "        p = to_probs_from_Q(q_list)\n",
    "        p = mix_with_epsilon(p, self.eps_flat)\n",
    "        return list(zip(flats, p))\n",
    "\n",
    "    def choose_flat(self, action: str, flats: Sequence[str]) -> str:\n",
    "        pairs = self.flat_probs(action, flats)\n",
    "        names, probs = zip(*pairs)\n",
    "        idx = sample_index(probs)\n",
    "        return names[idx]\n",
    "\n",
    "    def update_action(self, action: str, reward: float) -> None:\n",
    "        q = self.Q_action[action]\n",
    "        self.Q_action[action] = q + self.alpha * (reward - q)\n",
    "\n",
    "    def update_flat(self, action: str, flat: str, reward: float) -> None:\n",
    "        if flat not in self.Q_flat[action]:\n",
    "            self.Q_flat[action][flat] = self.init_q_flat\n",
    "        q = self.Q_flat[action][flat]\n",
    "        self.Q_flat[action][flat] = q + self.alpha * (reward - q)\n",
    "\n",
    "def run_smoke_test(steps: int = 30) -> None:\n",
    "    actions = load_actions_from_schema(SCHEMA_PATH)\n",
    "    policy = TwoStageTablePolicy(actions=actions)\n",
    "\n",
    "    print(\"Loaded actions (count):\", len(actions))\n",
    "    print(\"First 10 actions:\", actions[:10])\n",
    "\n",
    "    def show_action_probs(tag: str = \"\"):\n",
    "        pairs = policy.action_probs()\n",
    "        print(f\"[Action probs]{' ' + tag if tag else ''}:\")\n",
    "        for name, prob in pairs[:10]:  # show first 10 for brevity\n",
    "            print(f\"  - {name:20s} : {prob:.4f}\")\n",
    "        if len(pairs) > 10:\n",
    "            print(f\"  ... (+{len(pairs)-10} more)\")\n",
    "\n",
    "    show_action_probs(tag=\"(init)\")\n",
    "\n",
    "    for t in range(1, steps + 1):\n",
    "        a = policy.choose_action()\n",
    "        flat_candidates = [f\"{a}::candidate_{i}\" for i in range(1, 4)]\n",
    "        x = policy.choose_flat(a, flat_candidates)\n",
    "\n",
    "        r = random.uniform(-1.0, 1.0)\n",
    "\n",
    "        policy.update_action(a, r)\n",
    "        policy.update_flat(a, x, r)\n",
    "\n",
    "        if t % max(1, steps // 3) == 0:\n",
    "            print(f\"\\n--- Step {t}/{steps} ---\")\n",
    "            show_action_probs(tag=f\"after t={t}\")\n",
    "            fpairs = policy.flat_probs(a, [f\"{a}::candidate_{i}\" for i in range(1, 4)])\n",
    "            print(f\"[Flat probs for {a}] (example):\")\n",
    "            for name, prob in fpairs:\n",
    "                print(f\"  - {name:28s} : {prob:.4f}\")\n",
    "\n",
    "    print(\"\\n=== Final ===\")\n",
    "    show_action_probs(tag=\"(final)\")\n",
    "    for a in actions[:2]:\n",
    "        fpairs = policy.flat_probs(a, [f\"{a}::candidate_{i}\" for i in range(1, 4)])\n",
    "        print(f\"[Flat probs for {a}] (final sample):\")\n",
    "        for name, prob in fpairs:\n",
    "            print(f\"  - {name:28s} : {prob:.4f}\")\n",
    "\n",
    "# Execute immediately inside this environment\n",
    "run_smoke_test(steps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
